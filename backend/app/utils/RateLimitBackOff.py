import openai
from loguru import logger
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
    retry_if_exception_type,
)
from langchain_core.messages import BaseMessage
from langchain_openai import ChatOpenAI
from langchain_core.runnables import Runnable, RunnableConfig
from typing import List, Any

# å®šä¹‰éœ€è¦é‡è¯•çš„OpenAIå¼‚å¸¸ç±»å‹
RETRYABLE_EXCEPTIONS = (
    openai.APIError,
    openai.APITimeoutError,
    openai.APIConnectionError,
    openai.RateLimitError,
    openai.InternalServerError,
)

@retry(
    wait=wait_random_exponential(min=1, max=60),
    stop=stop_after_attempt(6),
    retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),
    before_sleep=lambda retry_state: logger.warning(
        f"ğŸš¨ LLM call failed, retrying... "
        f"Attempt: {retry_state.attempt_number}, "
        f"Wait: {retry_state.next_action.sleep:.2f}s, "
        f"Exception: {retry_state.outcome.exception()}"
    )
)
async def invoke_llm_with_backoff(llm: ChatOpenAI, messages: List[BaseMessage]) -> Any:
    """
    ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥å¼‚æ­¥è°ƒç”¨LLMã€‚
    å¦‚æœå‘ç”Ÿå¯é‡è¯•çš„APIé”™è¯¯ï¼Œå°†è‡ªåŠ¨é‡è¯•ã€‚
    """
    return await llm.ainvoke(messages)

@retry(
    wait=wait_random_exponential(min=1, max=60),
    stop=stop_after_attempt(6),
    retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),
    before_sleep=lambda retry_state: logger.warning(
        f"ğŸš¨ LLM call failed, retrying... "
        f"Attempt: {retry_state.attempt_number}, "
        f"Wait: {retry_state.next_action.sleep:.2f}s, "
        f"Exception: {retry_state.outcome.exception()}"
    )
)
def invoke_llm_sync_with_backoff(llm: ChatOpenAI, messages: List[BaseMessage]) -> Any:
    """
    ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥åŒæ­¥è°ƒç”¨LLMã€‚
    """
    return llm.invoke(messages)


@retry(
    wait=wait_random_exponential(min=1, max=60),
    stop=stop_after_attempt(6),
    retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),
    before_sleep=lambda retry_state: logger.warning(
        f"ğŸš¨ Chain call failed, retrying... "
        f"Attempt: {retry_state.attempt_number}, "
        f"Wait: {retry_state.next_action.sleep:.2f}s, "
        f"Exception: {retry_state.outcome.exception()}"
    )
)
async def invoke_chain_with_backoff(chain: Runnable, input_data: dict, config: RunnableConfig = None) -> Any:
    """
    ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥å¼‚æ­¥è°ƒç”¨LangChain chainã€‚
    å¦‚æœå‘ç”Ÿå¯é‡è¯•çš„APIé”™è¯¯ï¼Œå°†è‡ªåŠ¨é‡è¯•ã€‚
    """
    if config:
        return await chain.ainvoke(input_data, config)
    return await chain.ainvoke(input_data)
